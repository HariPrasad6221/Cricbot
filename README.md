# Cricbot
Computer-Vision-Based Robotic Batting System

This project integrates computer vision, embedded systems, and real-time control to design an autonomous robotic mechanism capable of detecting and striking an incoming object (ball) using live video input and Bluetooth-enabled actuation. The system combines Python (OpenCV) for visual tracking with an Arduino-controlled mechanical arm, demonstrating a seamless fusion of software-based perception and hardware-based motion response.

The vision module, developed in Python, utilizes the OpenCV library to process real-time images streamed from an Android IP Webcam over a local network. Each frame undergoes preprocessing—resizing, Gaussian blurring, and color-space conversion from BGR to HSV—to enhance detection accuracy under variable lighting. A color segmentation algorithm isolates the ball by filtering pixels within a tuned HSV range, followed by morphological operations (erosion and dilation) to remove noise. The largest contour is then identified, and its centroid and radius are computed, representing the ball’s position (X, Y) and apparent size R within the image.

As the object approaches the camera, its radius increases. This geometric behavior is used as a proximity cue—once R exceeds a threshold (rthresh), the system interprets it as the ball entering the strike zone. The positional data (X, Y, R) is transmitted via Bluetooth serial communication to the Arduino microcontroller, encoded in a concise, formatted packet for reliable parsing. The communication layer ensures real-time synchronization between the vision and control subsystems, enabling coordinated actuation.

On the Arduino side, a Servo motor and a DC motor control the mechanical movement of a bat-like arm. The servo is responsible for the swinging motion in the Z-axis, while the DC motor enables horizontal alignment in the X-axis. The microcontroller receives positional data, parses it, and maps the X coordinate to a time-based control signal that repositions the bat laterally toward the ball’s predicted impact point. Once the radius surpasses the defined threshold, a servo routine (hit()) is triggered, executing a swift swinging motion from 135° to 0° and back, simulating a realistic striking action. After impact, the system resets to its initial position, ready for the next cycle.

From a systems perspective, the project showcases real-time image processing, serial data transmission, and embedded actuation control working in tandem. The design emphasizes modularity—each subsystem (vision, communication, and actuation) can be tuned or upgraded independently. The Python module supports adaptive color calibration for different target colors, while the Arduino firmware allows parameter adjustments for motor speed, swing timing, and strike thresholds.

Overall, this project exemplifies applied mechatronics and embedded AI principles: transforming visual perception into dynamic mechanical action. It demonstrates proficiency in OpenCV, Python, Arduino C/C++, serial communication protocols, and servo/DC motor control, as well as skills in signal synchronization, hardware integration, and debugging across hardware-software boundaries. The final outcome is an intelligent robotic system capable of autonomously tracking, predicting, and responding to a moving physical object—bridging the gap between digital vision and physical actuation.
